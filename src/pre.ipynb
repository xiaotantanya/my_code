{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = importlib.import_module(\"torch.nn\")\n",
    "print(getattr(module,\"Module\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--f\",help=\"tre\")\n",
    "print(parser._action_groups[0])\n",
    "print(parser._action_groups[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForMaskedLM,BertTokenizer,BertModel,BertForMaskedLM\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "text = \"I am a student\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "masked_index = 2\n",
    "tokenized_text[masked_index] = \"[MASK]\"\n",
    "print(tokenized_text)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "output = model(tokens_tensor)\n",
    "print(output)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(100,100).clone().detach()\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = torch.nn.Linear(100,100,bias=True)\n",
    "        self.linear2 = torch.nn.Linear(100,10,bias=True)\n",
    "        self.linear3 = torch.nn.Linear(10,5,bias=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "    \n",
    "model = Model()\n",
    "print([name for name in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
      "        [0.5000, 0.5000, 0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.softmax(torch.Tensor(2,4),dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer):\n",
    "    # berts tokenize ways\n",
    "    # tokenize the [unused12345678910]\n",
    "    D = [f\"[unused{i}]\" for i in range(10)]\n",
    "    textraw = [text]\n",
    "    for delimiter in D:\n",
    "        ntextraw = []\n",
    "        for i in range(len(textraw)):\n",
    "            t = textraw[i].split(delimiter)\n",
    "            for j in range(len(t)):\n",
    "                ntextraw += [t[j]]\n",
    "                if j != len(t)-1:\n",
    "                    ntextraw += [delimiter]\n",
    "        textraw = ntextraw\n",
    "    text = []\n",
    "    for t in textraw:\n",
    "        if t in D:\n",
    "            text += [t]\n",
    "        else:\n",
    "            tokens = tokenizer.tokenize(t, add_special_tokens=False)\n",
    "            for tok in tokens:\n",
    "                text += [tok]\n",
    "\n",
    "    for idx, t in enumerate(text):\n",
    "        if idx + 3 < len(text) and t == \"[\" and text[idx+1] == \"[UNK]\" and text[idx+2] == \"]\":\n",
    "            text = text[:idx] + [\"[MASK]\"] + text[idx+3:]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'just', 'say', ',', 'you', 'are', 'right', ',', 'and', 'i', 'believe', 'you', 'forever']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "token = \"i just say ,you are right,and i believe you forever\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(tokenize(token,tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowprompt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
